{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('StackOverflow_cleaned.csv',sep=\";\", index_col=0,converters={\"Title\": literal_eval,\n",
    "                                                                                 \"Body\": literal_eval,\n",
    "                                                                                  \"Tags\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[0:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_stc(my_text) :\n",
    "    \n",
    "    transf_desc_text = ' '.join(my_text)\n",
    "    return transf_desc_text\n",
    "\n",
    "X =  data['Body'].apply(lambda x : transform_stc(x))\n",
    "y = data[\"Tags\"]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# transform output : \n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(y)\n",
    "y_bin = multilabel_binarizer.transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 586)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\",\n",
    "                             max_df=.6,\n",
    "                             min_df=0.005,\n",
    "                             tokenizer=None,\n",
    "                             #preprocessor=' '.join,\n",
    "                             stop_words=None,\n",
    "                             lowercase=False)\n",
    "\n",
    "\n",
    "vectorizer.fit(X)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build & train Word2Vec model ...\n",
      "Vocabulary size: 18826\n",
      "Word2Vec trained\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim\n",
    "import time\n",
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24 # adapt to length of sentences\n",
    "sentences = X.to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]\n",
    "\n",
    "\n",
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)\n",
    "\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Tokenizer ...\n",
      "Number of unique words: 18827\n"
     ]
    }
   ],
   "source": [
    "# Préparation des sentences (tokenization)\n",
    "\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') \n",
    "                                                   \n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding matrix ...\n",
      "Word embedding rate :  1.0\n",
      "Embedding matrix: (18827, 300)\n"
     ]
    }
   ],
   "source": [
    "# Création de la matrice d'embedding\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0\n",
    "    \n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "            \n",
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 24, 300)           5648100   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 300)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,648,100\n",
      "Trainable params: 5,648,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v= embed_model.predict(x_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "import os\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True, \n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "    \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0], \n",
    "                             bert_inp['token_type_ids'][0], \n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "    \n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size], \n",
    "                                                                      bert_tokenizer, max_length)\n",
    "        \n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids, \n",
    "                                 \"input_mask\" : attention_mask, \n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "             \n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "            last_hidden_states_tot_0 = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "    \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2)\n",
    "     \n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "#import tensorflow_text \n",
    "\n",
    "# Guide sur le Tensorflow hub : https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "model_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = hub.KerasLayer(model_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = bert_layer\n",
    "sentences = X.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Aeorn/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Aeorn/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\Aeorn/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Aeorn/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Aeorn/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temps traitement :  1410.0\n"
     ]
    }
   ],
   "source": [
    "# Création des features\n",
    "\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences, max_length, batch_size, mode='TFhub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert= features_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"universal-sentence-encoder_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size) :\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step ==0 :\n",
    "            features = feat\n",
    "        else :\n",
    "            features = np.concatenate((features,feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = X.to_list()\n",
    "\n",
    "X_USE = feature_USE_fct(sentences, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneVsRest Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_score(model, df, y_true, y_pred):\n",
    " \n",
    "    if(df is not None):\n",
    "        temp_df = df\n",
    "    else:\n",
    "        temp_df = pd.DataFrame(index=[\"Accuracy\", \"F1\",\n",
    "                                      \"Jaccard\", \"Recall\",\n",
    "                                      \"Precision\"],\n",
    "                               columns=[model])\n",
    "        \n",
    "    scores = []\n",
    "    scores.append(metrics.accuracy_score(y_true, \n",
    "                                         y_pred))\n",
    "    scores.append(metrics.f1_score(y_pred, \n",
    "                                   y_true, \n",
    "                                   average='weighted'))\n",
    "    scores.append(metrics.jaccard_score(y_true, \n",
    "                                        y_pred, \n",
    "                                        average='weighted'))\n",
    "    scores.append(metrics.recall_score(y_true, \n",
    "                                       y_pred, \n",
    "                                       average='weighted'))\n",
    "    scores.append(metrics.precision_score(y_true, \n",
    "                                          y_pred, \n",
    "                                          average='weighted'))\n",
    "    temp_df[model] = scores\n",
    "    \n",
    "    return temp_df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_run = [RandomForestClassifier(),LogisticRegression()]\n",
    "\n",
    "\n",
    "models_param_grid = [{\"estimator__max_depth\": [5, 25, 50],\n",
    "             \"estimator__min_samples_leaf\": [1, 5, 10],\n",
    "             \"estimator__class_weight\": [\"balanced\"]}, \n",
    "             \n",
    "             {\"estimator__C\": [100, 10, 1.0, 0.1],\n",
    "               \"estimator__penalty\": [\"l1\", \"l2\"],\n",
    "               \"estimator__dual\": [False],\n",
    "               \"estimator__solver\": [\"liblinear\"]}] \n",
    "\n",
    "inputs = [X_tfidf, X_w2v, X_bert,X_USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "domestic-check",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-13T05:53:29.410116Z",
     "iopub.status.busy": "2021-07-13T05:53:29.409435Z",
     "iopub.status.idle": "2021-07-13T05:57:39.420988Z",
     "shell.execute_reply": "2021-07-13T05:57:39.421525Z",
     "shell.execute_reply.started": "2021-07-12T13:10:56.3161Z"
    },
    "papermill": {
     "duration": 250.136813,
     "end_time": "2021-07-13T05:57:39.421711",
     "exception": false,
     "start_time": "2021-07-13T05:53:29.284898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "{'estimator__class_weight': 'balanced', 'estimator__max_depth': 25, 'estimator__min_samples_leaf': 5}\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "{'estimator__C': 10, 'estimator__dual': False, 'estimator__penalty': 'l1', 'estimator__solver': 'liblinear'}\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "{'estimator__class_weight': 'balanced', 'estimator__max_depth': 5, 'estimator__min_samples_leaf': 1}\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "{'estimator__C': 10, 'estimator__dual': False, 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "{'estimator__class_weight': 'balanced', 'estimator__max_depth': 5, 'estimator__min_samples_leaf': 5}\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "{'estimator__C': 10, 'estimator__dual': False, 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "{'estimator__class_weight': 'balanced', 'estimator__max_depth': 5, 'estimator__min_samples_leaf': 1}\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "{'estimator__C': 10, 'estimator__dual': False, 'estimator__penalty': 'l2', 'estimator__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize RandomForest with OneVsRest\n",
    "df_metrics_compare = pd.DataFrame()\n",
    "mod=0\n",
    "for bodies in inputs:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(bodies, y_bin, test_size=0.3, random_state=8)\n",
    "    for i,model in enumerate(models_to_run):\n",
    "        mod=mod+1\n",
    "        multi_cv = GridSearchCV(OneVsRestClassifier(model),\n",
    "                            param_grid=models_param_grid[i],\n",
    "                            n_jobs=-1,\n",
    "                            cv=2,\n",
    "                            scoring=\"f1_weighted\",\n",
    "                            return_train_score = True,\n",
    "                            refit=True,\n",
    "                            verbose=3)\n",
    "        # Fit on Sample data\n",
    "        multi_cv.fit(X_train, y_train)\n",
    "\n",
    "        rfc_cv_results = pd.DataFrame.from_dict(multi_cv.cv_results_)\n",
    "        rfc_best_params = multi_cv.best_params_\n",
    "        print(rfc_best_params)\n",
    "        rfc_best_params_ok = {}\n",
    "\n",
    "        for k, v in rfc_best_params.items():\n",
    "            rfc_best_params_ok[k.replace(\"estimator__\",\"\")] = v\n",
    "\n",
    "  \n",
    "        y_test_predicted_labels_tfidf_rfc = multi_cv.predict(X_test)\n",
    "\n",
    "        # Inverse transform\n",
    "        y_test_pred_inversed_rfc = multilabel_binarizer.inverse_transform(y_test_predicted_labels_tfidf_rfc)\n",
    "        y_test_inversed = multilabel_binarizer.inverse_transform(y_test)\n",
    "\n",
    "        metrics_score(\"Modele\"+str(mod), \n",
    "                                   df=df_metrics_compare,\n",
    "                                   y_true = y_test,\n",
    "                                   y_pred = y_test_predicted_labels_tfidf_rfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modele1</th>\n",
       "      <th>Modele2</th>\n",
       "      <th>Modele3</th>\n",
       "      <th>Modele4</th>\n",
       "      <th>Modele5</th>\n",
       "      <th>Modele6</th>\n",
       "      <th>Modele7</th>\n",
       "      <th>Modele8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.154667</td>\n",
       "      <td>0.200222</td>\n",
       "      <td>0.044222</td>\n",
       "      <td>0.173111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.258889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.524870</td>\n",
       "      <td>0.501880</td>\n",
       "      <td>0.389635</td>\n",
       "      <td>0.472199</td>\n",
       "      <td>0.317375</td>\n",
       "      <td>0.421554</td>\n",
       "      <td>0.492814</td>\n",
       "      <td>0.581257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380939</td>\n",
       "      <td>0.329457</td>\n",
       "      <td>0.266318</td>\n",
       "      <td>0.299416</td>\n",
       "      <td>0.197462</td>\n",
       "      <td>0.261695</td>\n",
       "      <td>0.378166</td>\n",
       "      <td>0.393110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.615163</td>\n",
       "      <td>0.394377</td>\n",
       "      <td>0.630727</td>\n",
       "      <td>0.366888</td>\n",
       "      <td>0.500690</td>\n",
       "      <td>0.331241</td>\n",
       "      <td>0.720095</td>\n",
       "      <td>0.461905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.496376</td>\n",
       "      <td>0.624840</td>\n",
       "      <td>0.317833</td>\n",
       "      <td>0.593134</td>\n",
       "      <td>0.248840</td>\n",
       "      <td>0.530105</td>\n",
       "      <td>0.449881</td>\n",
       "      <td>0.693119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Modele1   Modele2   Modele3   Modele4   Modele5   Modele6   Modele7  \\\n",
       "0  0.154667  0.200222  0.044222  0.173111  0.027778  0.145333  0.089333   \n",
       "1  0.524870  0.501880  0.389635  0.472199  0.317375  0.421554  0.492814   \n",
       "2  0.380939  0.329457  0.266318  0.299416  0.197462  0.261695  0.378166   \n",
       "3  0.615163  0.394377  0.630727  0.366888  0.500690  0.331241  0.720095   \n",
       "4  0.496376  0.624840  0.317833  0.593134  0.248840  0.530105  0.449881   \n",
       "\n",
       "    Modele8  \n",
       "0  0.258889  \n",
       "1  0.581257  \n",
       "2  0.393110  \n",
       "3  0.461905  \n",
       "4  0.693119  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_compare"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f7e7a8048ab9569ccd7edaade9a9d1227dad3950a1e5a7122e71aad3389e168"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
